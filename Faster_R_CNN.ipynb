{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpuWFq3DUnd4Tq27Y7Sjh0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaganKumar2375/Palm-Tree-Detection-using-YOLOv8-and-R-CNN/blob/main/Faster_R_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6z_gry2WN5bd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b4b8c8b"
      },
      "source": [
        "# Faster R-CNN Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b768f69"
      },
      "outputs": [],
      "source": [
        "# Install pyyaml and ensure PyTorch and torchvision are installed\n",
        "!pip install pyyaml\n",
        "!pip install --upgrade torch torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a2641ad"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.utils.data\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as F\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import cv2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5cf3e03"
      },
      "outputs": [],
      "source": [
        "# Install Roboflow library\n",
        "!pip install roboflow\n",
        "\n",
        "# Import Roboflow\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"A2wycrFsDr1E3JqnNRlT\")\n",
        "project = rf.workspace(\"capstone-p9zrm\").project(\"palm-tree-jkpzn\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov8\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c198d00",
        "outputId": "6daaca82-35a0-4cda-ba47-3f65e4a42acc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data.yaml  README.dataset.txt  README.roboflow.txt  test  train  valid\n"
          ]
        }
      ],
      "source": [
        "# List the contents of the dataset directory\n",
        "!ls {dataset.location}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77679e42"
      },
      "outputs": [],
      "source": [
        "class PalmTreeDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "\n",
        "        imgs = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n",
        "        labels = list(sorted(os.listdir(os.path.join(root, \"labels\"))))\n",
        "\n",
        "        # Filter out images without annotations\n",
        "        self.imgs = []\n",
        "        self.labels = []\n",
        "        for img_file, label_file in zip(imgs, labels):\n",
        "            label_path = os.path.join(root, \"labels\", label_file)\n",
        "            with open(label_path, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "                if not lines or all(line.strip() == '' for line in lines):\n",
        "                    continue  # Skip images with no annotations\n",
        "            self.imgs.append(img_file)\n",
        "            self.labels.append(label_file)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = np.array(img)\n",
        "        height, width = img.shape[:2]\n",
        "\n",
        "        # Load annotations\n",
        "        label_path = os.path.join(self.root, \"labels\", self.labels[idx])\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        with open(label_path, 'r') as f:\n",
        "            for line in f:\n",
        "                if line.strip() == '':\n",
        "                    continue\n",
        "                values = line.strip().split()\n",
        "                if len(values) != 5:\n",
        "                    continue  # Skip lines with incorrect format\n",
        "                class_id, x_center, y_center, bbox_width, bbox_height = map(float, values)\n",
        "                # Convert to pixel coordinates\n",
        "                x_center *= width\n",
        "                y_center *= height\n",
        "                bbox_width *= width\n",
        "                bbox_height *= height\n",
        "                x_min = x_center - bbox_width / 2\n",
        "                y_min = y_center - bbox_height / 2\n",
        "                x_max = x_center + bbox_width / 2\n",
        "                y_max = y_center + bbox_height / 2\n",
        "                boxes.append([x_min, y_min, x_max, y_max])\n",
        "                labels.append(int(class_id) + 1)  # +1 for background class\n",
        "\n",
        "        # **Handle empty boxes**\n",
        "        if len(boxes) == 0:\n",
        "            # Skip images with no annotations\n",
        "            return self.__getitem__((idx + 1) % len(self))\n",
        "\n",
        "        # Convert to tensors\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        # **Ensure boxes is 2D**\n",
        "        if boxes.ndim == 1:\n",
        "            boxes = boxes.unsqueeze(0)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        iscrowd = torch.zeros((len(labels),), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b241aa40"
      },
      "outputs": [],
      "source": [
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ea737c5"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import os\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "# Number of classes (including background)\n",
        "num_classes = 2  # 1 class (palm tree) + background\n",
        "\n",
        "# Get the number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "# Replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49ac218e"
      },
      "outputs": [],
      "source": [
        "# Define root directories for train and validation datasets\n",
        "train_dir = os.path.join(dataset.location, 'train')\n",
        "valid_dir = os.path.join(dataset.location, 'valid')\n",
        "\n",
        "# Create the datasets\n",
        "dataset_train = PalmTreeDataset(train_dir, transforms=get_transform(train=True))\n",
        "dataset_valid = PalmTreeDataset(valid_dir, transforms=get_transform(train=False))\n",
        "\n",
        "# Define data loaders\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "data_loader_train = DataLoader(dataset_train, batch_size=4, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
        "data_loader_valid = DataLoader(dataset_valid, batch_size=1, shuffle=False, num_workers=2, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bcfce75",
        "outputId": "9e88fb30-8ddc-46fa-a950-5bdfe16db9a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total images without annotations: 0\n"
          ]
        }
      ],
      "source": [
        "num_empty = 0\n",
        "for idx in range(len(dataset_train)):\n",
        "    _, target = dataset_train[idx]\n",
        "    if len(target['boxes']) == 0:\n",
        "        print(f\"Image at index {idx} has no annotations.\")\n",
        "        num_empty += 1\n",
        "print(f\"Total images without annotations: {num_empty}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec858275"
      },
      "outputs": [],
      "source": [
        "for img_file, label_file in zip(dataset_train.imgs, dataset_train.labels):\n",
        "    img_name = os.path.splitext(img_file)[0]\n",
        "    label_name = os.path.splitext(label_file)[0]\n",
        "    if img_name != label_name:\n",
        "        print(f\"Mismatch: {img_name} and {label_name}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8113be77"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# Learning rate scheduler (optional)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6847b8c"
      },
      "outputs": [],
      "source": [
        "dataset_train = PalmTreeDataset(train_dir, transforms=get_transform(train=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f2eb8f4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Access the image and label file names from the dataset instance\n",
        "imgs = dataset_train.imgs\n",
        "labels = dataset_train.labels\n",
        "\n",
        "# Iterate over the image and label files\n",
        "for img_file, label_file in zip(imgs, labels):\n",
        "    img_name = os.path.splitext(img_file)[0]\n",
        "    label_name = os.path.splitext(label_file)[0]\n",
        "    if img_name != label_name:\n",
        "        print(f\"Mismatch: {img_name} and {label_name}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b78a556"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "box_losses = []\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_box_loss = 0\n",
        "    i = 0\n",
        "\n",
        "    for images, targets in data_loader_train:\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        box_loss = loss_dict['loss_box_reg'].item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += losses.item()\n",
        "        epoch_box_loss += box_loss\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] Step [{i}/{len(data_loader_train)}] \"\n",
        "                  f\"Loss: {losses.item():.4f}, Box Loss: {box_loss:.4f}\")\n",
        "        i += 1\n",
        "\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(data_loader_train)\n",
        "    avg_box_loss = epoch_box_loss / len(data_loader_train)\n",
        "    train_losses.append(avg_epoch_loss)\n",
        "    box_losses.append(avg_box_loss)\n",
        "\n",
        "    print(f\"‚úÖ Epoch [{epoch+1}/{num_epochs}] Avg Loss: {avg_epoch_loss:.4f}, Avg Box Loss: {avg_box_loss:.4f}\")\n",
        "\n",
        "    # Validation step\n",
        "    model.train()  # üëà Temporarily switch to train mode to get loss dict\n",
        "    val_epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader_valid:\n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            val_loss_dict = model(images, targets)  # ‚úÖ Now returns a loss dict\n",
        "            val_losses_total = sum(loss for loss in val_loss_dict.values())\n",
        "            val_epoch_loss += val_losses_total.item()\n",
        "\n",
        "    model.eval()  # üëà Set it back to eval mode after validation\n",
        "\n",
        "\n",
        "    avg_val_loss = val_epoch_loss / len(data_loader_valid)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    print(f\"üîç Validation Loss after Epoch [{epoch+1}/{num_epochs}]: {avg_val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cdd5f90"
      },
      "outputs": [],
      "source": [
        "%pip install torchmetrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "497df6ac"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import torchvision.ops as ops\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# If you have multiple classes, extend this\n",
        "label_map = {\n",
        "    1: \"Palm Tree\"\n",
        "}\n",
        "\n",
        "def compute_iou(boxA, boxB):\n",
        "    \"\"\"Compute IoU between two boxes.\"\"\"\n",
        "    boxA = torch.tensor(boxA).unsqueeze(0)\n",
        "    boxB = torch.tensor(boxB).unsqueeze(0)\n",
        "    return ops.box_iou(boxA, boxB).item()\n",
        "\n",
        "\n",
        "def visualize_predictions(model, dataset, device, num_images=5, threshold=0.5):\n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "\n",
        "    total_gt = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    for i in range(num_images):\n",
        "        img, target = dataset[i]\n",
        "        img = img.to(device)\n",
        "        prediction = model([img])[0]\n",
        "\n",
        "        img_cpu = img.cpu()\n",
        "        pil_img = to_pil_image(img_cpu)\n",
        "        pred_boxes = prediction['boxes'].cpu().numpy()\n",
        "        pred_scores = prediction['scores'].cpu().numpy()\n",
        "        pred_labels = prediction['labels'].cpu().numpy()\n",
        "\n",
        "        gt_boxes = target['boxes'].numpy()\n",
        "        gt_labels = target['labels'].numpy()\n",
        "        total_gt += len(gt_labels)\n",
        "\n",
        "        gt_matched = set()\n",
        "        pred_matched = set()\n",
        "\n",
        "        for gt_idx, gt_box in enumerate(gt_boxes):\n",
        "            for pred_idx, (p_box, p_label, p_score) in enumerate(zip(pred_boxes, pred_labels, pred_scores)):\n",
        "                if p_score < threshold or pred_idx in pred_matched or gt_idx in gt_matched:\n",
        "                    continue\n",
        "                iou = compute_iou(gt_box, p_box)\n",
        "                if iou > 0.5 and p_label == gt_labels[gt_idx]:\n",
        "                    total_correct += 1\n",
        "                    gt_matched.add(gt_idx)\n",
        "                    pred_matched.add(pred_idx)\n",
        "                    break\n",
        "\n",
        "        # Visualization\n",
        "        fig, ax = plt.subplots(1, figsize=(12, 9))\n",
        "        ax.imshow(pil_img)\n",
        "\n",
        "        for box, score, label in zip(pred_boxes, pred_scores, pred_labels):\n",
        "            if score > threshold:\n",
        "                x_min, y_min, x_max, y_max = box\n",
        "                class_name = label_map.get(label, f\"Class {label}\")\n",
        "                rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
        "                                         linewidth=2, edgecolor='red', facecolor='none')\n",
        "                ax.add_patch(rect)\n",
        "                ax.text(x_min, y_min - 10, f'{class_name}: {score:.2f}', color='red', fontsize=12)\n",
        "\n",
        "        plt.title(f\"Prediction {i+1}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    accuracy = total_correct / total_gt if total_gt > 0 else 0.0\n",
        "    # print(f\"\\nDetection Accuracy (IoU > 0.5 & correct label): {accuracy:.2%}\")\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataset, device, threshold=0.5):\n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "\n",
        "    y_true_all = []\n",
        "    y_pred_all = []\n",
        "\n",
        "    total_tp = 0\n",
        "    total_fp = 0\n",
        "    total_fn = 0\n",
        "    num_images = len(dataset)\n",
        "\n",
        "    for i in range(num_images):\n",
        "        img, target = dataset[i]\n",
        "        img = img.to(device)\n",
        "        output = model([img])[0]\n",
        "\n",
        "        pred_boxes = output['boxes'].cpu().numpy()\n",
        "        pred_scores = output['scores'].cpu().numpy()\n",
        "        pred_labels = output['labels'].cpu().numpy()\n",
        "\n",
        "        gt_boxes = target['boxes'].numpy()\n",
        "        gt_labels = target['labels'].numpy()\n",
        "\n",
        "        matched_gt = set()\n",
        "        matched_pred = set()\n",
        "\n",
        "        for pred_idx, (p_box, p_label, p_score) in enumerate(zip(pred_boxes, pred_labels, pred_scores)):\n",
        "            if p_score < threshold:\n",
        "                continue\n",
        "            for gt_idx, (gt_box, gt_label) in enumerate(zip(gt_boxes, gt_labels)):\n",
        "                if gt_idx in matched_gt or pred_idx in matched_pred:\n",
        "                    continue\n",
        "                iou = compute_iou(gt_box, p_box)\n",
        "                if iou > 0.5:\n",
        "                    y_true_all.append(gt_label)\n",
        "                    y_pred_all.append(p_label)\n",
        "                    total_tp += 1\n",
        "                    matched_gt.add(gt_idx)\n",
        "                    matched_pred.add(pred_idx)\n",
        "                    break\n",
        "            else:\n",
        "                total_fp += 1  # unmatched prediction\n",
        "\n",
        "        total_fn += len(gt_boxes) - len(matched_gt)\n",
        "\n",
        "    # Classification metrics\n",
        "    precision = precision_score(y_true_all, y_pred_all, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_true_all, y_pred_all, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_true_all, y_pred_all, average='weighted', zero_division=0)\n",
        "\n",
        "    print(f\"\\nModel Evaluation (IoU > 0.5 & correct class):\")\n",
        "    print(f\"Precision: {precision:.2%}\")\n",
        "    print(f\"Recall:    {recall:.2%}\")\n",
        "    print(f\"F1 Score:  {f1:.2%}\")\n",
        "    print(f\"TP: {total_tp}, FP: {total_fp}, FN: {total_fn}\")\n",
        "\n",
        "    # Confusion matrix\n",
        "    if len(set(y_true_all)) > 0:\n",
        "        labels = list(label_map.keys())  # [0, 1]\n",
        "        cm = confusion_matrix(y_true_all, y_pred_all, labels=labels)\n",
        "        display_labels = [label_map[lbl] for lbl in labels]\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n",
        "\n",
        "        disp.plot(cmap='Blues')\n",
        "        plt.title(\"Confusion Matrix\")\n",
        "        plt.show()\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "\n",
        "def compute_map(model, dataset, device, threshold=0.5):\n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "\n",
        "    metric = MeanAveragePrecision()\n",
        "    for i in range(len(dataset)):\n",
        "        img, target = dataset[i]\n",
        "        img = img.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = model([img])[0]\n",
        "\n",
        "        # Format predictions\n",
        "        pred_formatted = [{\n",
        "            \"boxes\": pred[\"boxes\"].cpu(),\n",
        "            \"scores\": pred[\"scores\"].cpu(),\n",
        "            \"labels\": pred[\"labels\"].cpu()\n",
        "        }]\n",
        "\n",
        "        # Format ground truth\n",
        "        target_formatted = [{\n",
        "            \"boxes\": target[\"boxes\"],\n",
        "            \"labels\": target[\"labels\"]\n",
        "        }]\n",
        "\n",
        "        metric.update(pred_formatted, target_formatted)\n",
        "\n",
        "    results = metric.compute()\n",
        "    print(f\"\\n‚úÖ Mean Average Precision (mAP@0.5:0.95): {results['map']:.4f}\")\n",
        "    print(f\"‚úÖ Mean Average Precision (mAP@0.5): {results['map_50']:.4f}\")\n",
        "\n",
        "# Show prediction results on sample images\n",
        "visualize_predictions(model, dataset_valid, device, num_images=5, threshold=0.5)\n",
        "compute_map(model, dataset_valid, device)\n",
        "\n",
        "# Evaluate model performance across all data\n",
        "evaluate_model(model, dataset_valid, device)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eac105b4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, marker='o', linestyle='-')\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d7bc49b"
      },
      "outputs": [],
      "source": [
        "# Save the model's state dictionary\n",
        "torch.save(model.state_dict(), 'faster_rcnn_palm_tree.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccd6c17c",
        "outputId": "1c40b35c-38d1-4ba5-8569-0c7a316e0aa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Copy the model to Google Drive\n",
        "!cp faster_rcnn_palm_tree.pth /content/drive/MyDrive/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94fbb050"
      },
      "outputs": [],
      "source": [
        "# Load the model structure\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None)\n",
        "\n",
        "# Modify the model for your number of classes\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "num_classes = 2  # 1 class (palm tree) + background\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "# Load the saved state dictionary\n",
        "model.load_state_dict(torch.load('faster_rcnn_palm_tree.pth'))\n",
        "\n",
        "# Move the model to the device\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9cf5e7a"
      },
      "outputs": [],
      "source": [
        "# Load from Google Drive\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/faster_rcnn_palm_tree.pth'))\n"
      ]
    }
  ]
}